# -*- coding: utf-8 -*-
"""Copy of Untitled.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qIf1Akp3ZVHdeNJhEumwxIJI6Z4zfPgW
"""

#all in one

#ws1
#qn1
from PIL import Image
img=Image.open("/content/drive/MyDrive/images/image9.webp")
print(img.size)
for i in range(0, img.size[0]-1):
    for j in range(0, img.size[1]-1):
        pixelColorVals = img.getpixel((i,j));
        # Invert color
        redPixel    = 255 - pixelColorVals[0]; # Negate red pixe
        greenPixel  = 255 - pixelColorVals[1]; # Negate green pixel
        bluePixel   = 255 - pixelColorVals[2]; # Negate blue pixel
        # Modify the image with the inverted pixel values
        img.putpixel((i,j),(redPixel, greenPixel, bluePixel));
# Display the negative image
img

#qn2
from PIL import Image
img=Image.open("/content/drive/MyDrive/images/image1.png")
img=img.convert("L")
BW=img.convert('1')
BW

#qn3
#qn3
# Import necessary libraries
import cv2
from google.colab.patches import cv2_imshow
path = r'/content/drive/MyDrive/images/sample.mp4'
i = 1
video = cv2.VideoCapture(path)
while True:
	ret, img = video.read()
	cv2_imshow(img)
	if i>2:
		break
	filename = 'Frames '+str(i)+'.jpg'
	# Save the images in given path
	cv2.imwrite(filename, img)
	i = i+1
# close the camera
video.release()
# close open windows
cv2.destroyAllWindows()

#qn4

img1=Image.open("/content/Frames 1.jpg")
img2=Image.open("/content/Frames 2.jpg")
from PIL import ImageChops
img1 = img1.resize((1080,720))
img2 = img2.resize((1080,720))
img = ImageChops.difference(img1 , img2 )
img


#qn5
img1=Image.open("/content/drive/MyDrive/images/image1.png")
img2=Image.open("/content/drive/MyDrive/images/image2.png")
img1 = img1.resize((1080,720))
img2 = img2.resize((1080,720))
img = Image.blend(img1 , img2 , 0.5)
img


#qn 6
from PIL import ImageChops
img1 = img1.resize((1080,720))
img2 = img2.resize((1080,720))
img = ImageChops.difference(img1 , img2 )
img


#qn 7

#qn7a
img = img2.transpose(method= Image.AFFINE )
img

#qn7a
from PIL import Image
#qn7a
from PIL import Image
a = 1
b = 2
c = 0 
d = 2
e = 1
f = 0 
img = img1.transform(img.size, Image.AFFINE, (a, b, c, d, e, f))
img
#qn 7b
rotated_image = img1.rotate(45)
rotated_image

#qn 7c
resized_image = img2.resize((100,100))
resized_image

#qn8
from PIL import Image
img = Image.open("/content/drive/MyDrive/images/image1.png")
width, height = img.size
left = 100
top =0.2*height
right = 200
bottom =width-100
cropped_image = img.crop((left, top, right, bottom))
cropped_image

#qn9
import numpy as np
arr = np.array(img) 
for i in range(len(arr)) :
  for j in range(len(arr[0])) :
    if arr[i][j][0] >= 0 and arr[i][j][0] < 64:
      arr[i][j][0] = arr[i][j][1] = arr[i][j][2] = 32
    elif arr[i][j][0] >= 64 and arr[i][j][0] < 128:
      arr[i][j][0] = arr[i][j][1] = arr[i][j][2] = 96
    elif arr[i][j][0] >= 128 and arr[i][j][0] < 196:
      arr[i][j][0] = arr[i][j][1] = arr[i][j][2] = 162
    else:
      arr[i][j][0] = arr[i][j][1] = arr[i][j][2] = 255
img = Image.fromarray(arr)
img

#qn10

#10a
from PIL import ImageChops
im1 = Image.open("/content/drive/MyDrive/images/image9.webp") .convert("1")
im2 = Image.open("/content/drive/MyDrive/images/image1.png") .convert("1")
im3 = ImageChops.logical_and(im1, im2)
im3


#10b
im1 = Image.open("/content/drive/MyDrive/images/image9.webp") .convert("1")
im2 = Image.open("/content/drive/MyDrive/images/image1.png") .convert("1")
im3 = ImageChops.logical_or(im1, im2)
im3

#10c
im1 = Image.open("/content/drive/MyDrive/images/image9.webp") .convert("1")
im1




#ws connected component
import numpy as np
from matplotlib import pyplot as plt
imagea=np.random.randint(0,2,(8,8))

plt.gray()
plt.imshow(imagea)
plt.show()

def four_connectivity_labelling(image_array):
  labels = np.zeros([np.size(image_array, 0), np.size(image_array, 1)], dtype = int)
  label_count = 0
  object_count_deduct = 0
  equivalence_table= set()
  for i in range(len(image_array)):
    for j in range(len(image_array[i])):
     if image_array[i][j]==1:
       if i == 0 and j == 0:
        label_count+=1
        labels[i][j] = label_count
       elif i > 0 and j == 0:
        if image_array[i-1][j] == 1:
          labels[i][j] = labels[i-1][j]
        else:
          label_count+=1
          labels[i][j] = label_count
       elif i == 0 and j > 0:
        if image_array[i][j-1] == 1:
          labels[i][j] = labels[i][j-1]
        else:
          label_count+=1
          labels[i][j] = label_count
       else:
        if image_array[i-1][j] == 0 and image_array[i][j-1] == 1:
          labels[i][j] = labels[i][j-1]
        elif image_array[i-1][j] == 1 and image_array[i][j-1] == 0:
          labels[i][j] = labels[i-1][j]
        elif image_array[i][j-1] == 1 and image_array[i-1][j] == 1:
          labels[i][j] = min(labels[i-1][j], labels[i][j-1])
          if labels[i][j-1] != labels[i-1][j]:
            equivalence_table.add(max(labels[i-1][j], labels[i][j-1]))
        else:
          label_count+=1
          labels[i][j] = label_count

  print(labels)

four_connectivity_labelling(imagea)

#for real image 
import numpy as np
from matplotlib import pyplot as plt
from PIL import Image

image_m=Image.open("/content/figure.png")
image_m
image_m= image_m.convert('L')
image_m
BW= image_m.convert('1')
BW
img_arr = np.array(BW)
def four_connectivity_labelling(image_array):
  labels = np.zeros([np.size(image_array, 0), np.size(image_array, 1)], dtype = int)
  label_count = 0
  object_count_deduct = 0
  equivalence_table= set()
  for i in range(len(image_array)):
    for j in range(len(image_array[i])):
     if image_array[i][j]==1:
       if i == 0 and j == 0:
        label_count+=1
        labels[i][j] = label_count
       elif i > 0 and j == 0:
        if image_array[i-1][j] == 1:
          labels[i][j] = labels[i-1][j]
        else:
          label_count+=1
          labels[i][j] = label_count
       elif i == 0 and j > 0:
        if image_array[i][j-1] == 1:
          labels[i][j] = labels[i][j-1]
        else:
          label_count+=1
          labels[i][j] = label_count
       else:
        if image_array[i-1][j] == 0 and image_array[i][j-1] == 1:
          labels[i][j] = labels[i][j-1]
        elif image_array[i-1][j] == 1 and image_array[i][j-1] == 0:
          labels[i][j] = labels[i-1][j]
        elif image_array[i][j-1] == 1 and image_array[i-1][j] == 1:
          labels[i][j] = min(labels[i-1][j], labels[i][j-1])
          if labels[i][j-1] != labels[i-1][j]:
            equivalence_table.add(max(labels[i-1][j], labels[i][j-1]))
        else:
          label_count+=1
          labels[i][j] = label_count

  print(labels)
  # print(equivalence_table)
  # print("\nNumber of image objects are :- ", label_count - len(equivalence_table))

four_connectivity_labelling(img_arr)


#q2
import cv2
import numpy as np
import matplotlib.pyplot as plt

def connected_component_label(path):
    
    # Getting the input image
    img = cv2.imread(path, 0)
    # Converting those pixels with values 1-127 to 0 and others to 1
    img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)[1]
    # Applying cv2.connectedComponents() 
    num_labels, labels = cv2.connectedComponents(img)
    
    # Map component labels to hue val, 0-179 is the hue range in OpenCV
    label_hue = np.uint8(179*labels/np.max(labels))
    blank_ch = 255*np.ones_like(label_hue)
    labeled_img = cv2.merge([label_hue, blank_ch, blank_ch])

    # Converting cvt to BGR
    labeled_img = cv2.cvtColor(labeled_img, cv2.COLOR_HSV2BGR)

    # set bg label to black
    labeled_img[label_hue==0] = 0
    
    
    # Showing Original Image
    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.axis("off")
    plt.title("Orginal Image")
    plt.show()
    
    #Showing Image after Component Labeling
    plt.imshow(cv2.cvtColor(labeled_img, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.title("Image after Component Labeling")
    plt.show()
connected_component_label('/content/images.png')





#ws noise andannotation
import numpy as np
import cv2
from google.colab.patches import cv2_imshow

def add_salt_pepper_noise(image, prob):
    output = np.copy(image)
    thres = 1 - prob
    for i in range(image.shape[0]):
        for j in range(image.shape[1]):
            rdn = np.random.random()
            if rdn < prob:
                output[i][j] = 0
            elif rdn > thres:
                output[i][j] = 255
    return output

img = cv2.imread("bird.jpg")
print("\nOriginal Image\n")
cv2_imshow(img)
noisy_img = add_salt_pepper_noise(img, 0.05)
cv2.imwrite("noisy_salt_and_pepper_image.jpg", noisy_img)
print("\nSalt and pepper Noisy image\n")
cv2_imshow(noisy_img)

# Use Skimage library and generate Gaussian noise, Poisson noise, Salt and Pepper noise. Add these noise images to the original image and store them in different names.

import skimage
from skimage import io
import numpy as np
import os
from skimage import img_as_ubyte

img = io.imread("dog.jpg")
io.imshow(img)

def add_gaussian_noise(image, prob):
    gaussian_noise = skimage.util.random_noise(image, mode='gaussian', var=prob)
    io.imsave("gaussian_noise.jpg",img_as_ubyte(img))
    io.imshow(gaussian_noise)


print("\nGaussian Noise : \n")
add_gaussian_noise(img, 0.05)

def add_poisson_noise(image, prob):
    poisson_noise = skimage.util.random_noise(image, mode='poisson')
    io.imsave("poisson_noise.jpg", img_as_ubyte(poisson_noise))
    io.imshow(poisson_noise)

print("\nPoisson Noise : \n")
add_poisson_noise(img, 0.05)

def add_saltandpepper_noise(image, prob):
    salt_pepper_noise = skimage.util.random_noise(image, mode='s&p', amount=0.05)
    io.imsave("salt_pepper_noise.jpg", img_as_ubyte(salt_pepper_noise))
    io.imshow(salt_pepper_noise)

print("\nSalt and Pepper Noise : \n")
add_saltandpepper_noise(img, 0.05)


# For the newly generated images, apply mean filter, median filter and analyze the effectiveness.

def analyze_filters(img_paths):
    for img_path in img_paths:
        img = cv2.imread(img_path)
        
        # Mean filter
        mean_filter = cv2.blur(img,(3,3))
        mean_psnr = cv2.PSNR(img, mean_filter)
        print(f"PSNR for {img_path} with mean filter: {mean_psnr}")

        # Median filter
        median_filter = cv2.medianBlur(img,3)
        median_psnr = cv2.PSNR(img, median_filter)
        print(f"PSNR for {img_path} with median filter: {median_psnr}")

# Example usage
img_paths = ["gaussian_noise.jpg", "poisson_noise.jpg", "salt_pepper_noise.jpg"]
analyze_filters(img_paths)

#Text on Images

img = cv2.imread("flower.jpg")
imageText = img.copy()
text = 'This is a sample text'
org = (30,70)

# Text on the input image
cv2.putText(imageText, text, org, fontFace = cv2.FONT_HERSHEY_COMPLEX, fontScale = 0.8, color = (0,0,0))

cv2_imshow(imageText)

#RECTANGLE

imageRectangle = img.copy()

# starting and end points of the rectangle
start_point =(200,75)
end_point =(475,225)

cv2.rectangle(imageRectangle, start_point, end_point, (0, 0, 0), thickness= 3, lineType=cv2.LINE_8) 

cv2_imshow(imageRectangle)


#circle
imageCircle = img.copy()
imageFilledCircle = img.copy()
circle_center = (175,90)
radius =80
print("\nCircle\n")
cv2.circle(imageCircle, circle_center, radius, (0, 0, 0), thickness=3, lineType=cv2.LINE_AA) 
cv2_imshow(imageCircle)
print("\nFilled circle\n")
cv2.circle(imageFilledCircle, circle_center, radius, (0, 0, 0), thickness=-1, lineType=cv2.LINE_AA)
cv2_imshow(imageFilledCircle)


#line
imageLine = img.copy()
pointA = (5,220)
pointB = (250,220)
cv2.line(imageLine, pointA, pointB, (0, 0, 0), thickness=3)
cv2_imshow(imageLine)


#ellipse
imageEllipse = img.copy()
ellipse_center = (415,190)
axis1 = (75,40)
axis2 = (100,40)
cv2.ellipse(imageEllipse, ellipse_center, axis1, 0, 0, 360, (170, 51, 106), thickness=3)
cv2.ellipse(imageEllipse, ellipse_center, axis2, 90, 0, 360, (144, 238, 144), thickness=3)
cv2_imshow(imageEllipse)


#halfellipse
halfEllipse = img.copy()
ellipse_center = (315,190)

axis1 = (100,50)

cv2.ellipse(halfEllipse, ellipse_center, axis1, 0, 180, 360, (170, 51, 106), thickness=3)
cv2.ellipse(halfEllipse, ellipse_center, axis1, 0, 0, 180, (144, 238, 144), thickness=-2)
cv2_imshow(halfEllipse)






#ws color spacetransform and convolution

from PIL import Image
from google.colab import drive
drive.mount('/content/drive')

import cv2
from google.colab.patches import cv2_imshow
bright = cv2.imread('/content/drive/MyDrive/cube.png')
dark = cv2.imread('/content/drive/MyDrive/dark_cube.png')
cv2_imshow(bright)
print("\n")
cv2_imshow(dark)
#rgb to lab
brightLAB = cv2.cvtColor(bright, cv2.COLOR_BGR2LAB)
darkLAB = cv2.cvtColor(dark, cv2.COLOR_BGR2LAB)
cv2_imshow(brightLAB)
print("\n")
cv2_imshow(darkLAB)

#RGB To YCrCb
brightYCB = cv2.cvtColor(bright, cv2.COLOR_BGR2YCrCb)
darkYCB = cv2.cvtColor(dark, cv2.COLOR_BGR2YCrCb)
cv2_imshow(brightLAB)
print("\n")
cv2_imshow(darkLAB)


#RGB To HSV

brightHSV = cv2.cvtColor(bright, cv2.COLOR_BGR2HSV)
darkHSV = cv2.cvtColor(dark, cv2.COLOR_BGR2HSV)
cv2_imshow(brightLAB)
print("\n")
cv2_imshow(darkLAB)


#Create an array of size 15 x 15 containing gray intensity values. Consider another array of size 3 x 3 consisting of all 1's. Write a program to apply convolution.
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

image_array = np.random.randint(0, 255, (15,15))
filter_array = np.ones([3, 3], dtype = int)

plt.imshow(image_array)
plt.show()

convoluted_array = np.zeros([13, 13], dtype = int)

#mean filter
def convolute_image(image_array, i,j):
  sum=0
  for x in range(i- int(filter_array.shape[0]/2),i+ int(filter_array.shape[0]/2)):
    for y in range(j- int(filter_array.shape[1]/2),j+ int(filter_array.shape[1]/2)):
      sum+=image_array[x][y]
  return sum

for i in range(1, image_array.shape[0]-filter_array.shape[0]+2):
  for j in range(1, image_array.shape[1]-filter_array.shape[1]+2):
    convoluted_array[i-1][j-1] = convolute_image(image_array, i ,j)
plt.imshow(convoluted_array)
plt.show()

#edge detection


from google.colab.patches import cv2_imshow
import cv2

img = cv2.imread('/content/drive/MyDrive/dog.jpg') 
cv2_imshow(img)
cv2.waitKey(0)
img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

img_blur = cv2.GaussianBlur(img_gray, (3,3), 0) 
sobelx = cv2.Sobel(src=img_blur, ddepth=cv2.CV_64F, dx=1, dy=0, ksize=5) 
sobely = cv2.Sobel(src=img_blur, ddepth=cv2.CV_64F, dx=0, dy=1, ksize=5)
sobelxy = cv2.Sobel(src=img_blur, ddepth=cv2.CV_64F, dx=1, dy=1, ksize=5) 

cv2_imshow(sobelx)
cv2.waitKey(0)
cv2_imshow(sobely)
cv2.waitKey(0)
cv2_imshow(sobelxy)
cv2.waitKey(0)
edges = cv2.Canny(image=img_blur, threshold1=100, threshold2=200) 

cv2_imshow(edges)
cv2.waitKey(0)
cv2.destroyAllWindows()




#Canny edge detection

import numpy as np
import cv2
from google.colab.patches import cv2_imshow

image = cv2.imread("/content/drive/MyDrive/data/dog.jpg", cv2.IMREAD_GRAYSCALE)

"""# Gaussian Blurring"""

def gaussian(x, y, sigma):
  return (1 / (2 * np.pi * sigma ** 2)) * np.exp(-((x**2 + y**2) / (2 * sigma ** 2)))

def gaussian_kernel(size, sigma):
  k = size // 2
  kernel = np.zeros((size, size))
  for i in range(2 * k + 1):
    for j in range(2 * k + 1):
      kernel[i][j] = gaussian(i - k, j - k, sigma)
  
  return kernel

kernel = gaussian_kernel(5, 1)
image_gaussian = cv2.filter2D(src=image, ddepth=-1, kernel=kernel)

"""# Applying filter"""

image_sobel_x = cv2.filter2D(image_gaussian, ddepth=-1, kernel=np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]))
image_sobel_y = cv2.filter2D(image_gaussian, ddepth=-1, kernel=np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]))

"""# Non max suppression"""

magnitude = np.hypot(image_sobel_x, image_sobel_y)
magnitude = (magnitude / magnitude.max()) * 255
direction = np.arctan2(image_sobel_y, image_sobel_x)
direction = direction * 180 / np.pi
direction[direction < 0] += 180

cv2_imshow(magnitude)

image_suppressed = np.zeros((image.shape[0], image.shape[1]))
angle_direction_mapping = {
    (0, 22.5): (0, 1),
    (22.5, 67.5): (-1, -1),
    (67.5, 112.5): (1, 0),
    (112.5, 157.5): (1, -1),
    (157.5, 180): (0, 1)
}

for i in range(1, image.shape[0] - 1):
  for j in range(1, image.shape[1] - 1):
    current_pixel_value = magnitude[i][j]
    for angle, neighbour_direction in angle_direction_mapping.items():
      if angle[0] <= direction[i][j] < angle[1]:
        break
    neighbour1 = magnitude[i + neighbour_direction[0]][j + neighbour_direction[1]]
    neighbour2 = magnitude[i - neighbour_direction[0]][j - neighbour_direction[1]]
    if current_pixel_value >= neighbour1 and current_pixel_value >= neighbour2:
      image_suppressed[i][j] = current_pixel_value
    else:
      image_suppressed[i][j] = 0

# cv2_imshow((image_suppressed / image_suppressed.max()) * 255)
cv2_imshow(image_suppressed)

"""# Double Thresolding"""

low_thresh_ratio = 0.05
high_thresh_ratio = 0.09

STRONG = 255
WEAK = 25

high_thresh = image_suppressed.max() * high_thresh_ratio
low_thresh = high_thresh * low_thresh_ratio

thresholded_result = np.zeros(image.shape)
thresholded_result[image_suppressed > high_thresh] = STRONG
thresholded_result[(image_suppressed <= high_thresh) & (image_suppressed >= low_thresh)] = WEAK

cv2_imshow(thresholded_result)

"""# Edge tracking by Hysterisis"""

for i in range(1, image.shape[0]):
  for j in range(1, image.shape[1]):
    if thresholded_result[i][j] == WEAK:
      if (thresholded_result[i - 1:i + 2, j - 1 : j + 2] == STRONG).any():
        thresholded_result[i][j] = STRONG

cv2_imshow(thresholded_result)






#PCA

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import cv2
from scipy.stats import stats
import matplotlib.image as mpimg


img = cv2.cvtColor(cv2.imread('rose.jpg'), cv2.COLOR_BGR2RGB)
plt.imshow(img)
plt.show()

#Splitting into channels
blue,green,red = cv2.split(img)
# Plotting the images
fig = plt.figure(figsize = (15, 7.2)) 
fig.add_subplot(131)
plt.title("Blue Channel")
plt.imshow(blue)
fig.add_subplot(132)
plt.title("Green Channel")
plt.imshow(green)
fig.add_subplot(133)
plt.title("Red Channel")
plt.imshow(red)
plt.show()



blue_temp_df = pd.DataFrame(data = blue)
blue_temp_df

df_blue = blue/255
df_green = green/255
df_red = red/255


pca_b = PCA(n_components=50)
pca_b.fit(df_blue)
trans_pca_b = pca_b.transform(df_blue)
pca_g = PCA(n_components=50)
pca_g.fit(df_green)
trans_pca_g = pca_g.transform(df_green)
pca_r = PCA(n_components=50)
pca_r.fit(df_red)
trans_pca_r = pca_r.transform(df_red)


print(trans_pca_b.shape)
print(trans_pca_r.shape)
print(trans_pca_g.shape)


print(f"Blue Channel : {sum(pca_b.explained_variance_ratio_)}")
print(f"Green Channel: {sum(pca_g.explained_variance_ratio_)}")
print(f"Red Channel  : {sum(pca_r.explained_variance_ratio_)}")


fig = plt.figure(figsize = (15, 7.2)) 
fig.add_subplot(131)
plt.title("Blue Channel")
plt.ylabel('Variation explained')
plt.xlabel('Eigen Value')
plt.bar(list(range(1,51)),pca_b.explained_variance_ratio_)
fig.add_subplot(132)
plt.title("Green Channel")
plt.ylabel('Variation explained')
plt.xlabel('Eigen Value')
plt.bar(list(range(1,51)),pca_g.explained_variance_ratio_)
fig.add_subplot(133)
plt.title("Red Channel")
plt.ylabel('Variation explained')
plt.xlabel('Eigen Value')
plt.bar(list(range(1,51)),pca_r.explained_variance_ratio_)
plt.show()


#reconstruct
b_arr = pca_b.inverse_transform(trans_pca_b)
g_arr = pca_g.inverse_transform(trans_pca_g)
r_arr = pca_r.inverse_transform(trans_pca_r)
print(b_arr.shape, g_arr.shape, r_arr.shape)


img_reduced= (cv2.merge((b_arr, g_arr, r_arr)))
print(img_reduced.shape)


fig = plt.figure(figsize = (10, 7.2)) 
fig.add_subplot(121)
plt.title("Original Image")
plt.imshow(img)
fig.add_subplot(122)
plt.title("Reduced Image")
plt.imshow(img_reduced)
plt.show()

#ws1
#qn1
from PIL import Image
img=Image.open("/content/drive/MyDrive/images/image9.webp")
print(img.size)
for i in range(0, img.size[0]-1):
    for j in range(0, img.size[1]-1):
        pixelColorVals = img.getpixel((i,j));
        # Invert color
        redPixel    = 255 - pixelColorVals[0]; # Negate red pixe
        greenPixel  = 255 - pixelColorVals[1]; # Negate green pixel
        bluePixel   = 255 - pixelColorVals[2]; # Negate blue pixel
        # Modify the image with the inverted pixel values
        img.putpixel((i,j),(redPixel, greenPixel, bluePixel));
# Display the negative image
img

#qn2
from PIL import Image
img=Image.open("/content/drive/MyDrive/images/image1.png")
img=img.convert("L")
BW=img.convert('1')
BW

#qn3
#qn3
# Import necessary libraries
import cv2
from google.colab.patches import cv2_imshow
path = r'/content/drive/MyDrive/images/sample.mp4'
i = 1
video = cv2.VideoCapture(path)
while True:
	ret, img = video.read()
	cv2_imshow(img)
	if i>2:
		break
	filename = 'Frames '+str(i)+'.jpg'
	# Save the images in given path
	cv2.imwrite(filename, img)
	i = i+1
# close the camera
video.release()
# close open windows
cv2.destroyAllWindows()

#qn4

img1=Image.open("/content/Frames 1.jpg")
img2=Image.open("/content/Frames 2.jpg")
from PIL import ImageChops
img1 = img1.resize((1080,720))
img2 = img2.resize((1080,720))
img = ImageChops.difference(img1 , img2 )
img


#qn5
img1=Image.open("/content/drive/MyDrive/images/image1.png")
img2=Image.open("/content/drive/MyDrive/images/image2.png")
img1 = img1.resize((1080,720))
img2 = img2.resize((1080,720))
img = Image.blend(img1 , img2 , 0.5)
img


#qn 6
from PIL import ImageChops
img1 = img1.resize((1080,720))
img2 = img2.resize((1080,720))
img = ImageChops.difference(img1 , img2 )
img


#qn 7

#qn7a
img = img2.transpose(method= Image.AFFINE )
img

#qn7a
from PIL import Image
#qn7a
from PIL import Image
a = 1
b = 2
c = 0 
d = 2
e = 1
f = 0 
img = img1.transform(img.size, Image.AFFINE, (a, b, c, d, e, f))
img
#qn 7b
rotated_image = img1.rotate(45)
rotated_image

#qn 7c
resized_image = img2.resize((100,100))
resized_image

#qn8
from PIL import Image
img = Image.open("/content/drive/MyDrive/images/image1.png")
width, height = img.size
left = 100
top =0.2*height
right = 200
bottom =width-100
cropped_image = img.crop((left, top, right, bottom))
cropped_image

#qn9
import numpy as np
arr = np.array(img) 
for i in range(len(arr)) :
  for j in range(len(arr[0])) :
    if arr[i][j][0] >= 0 and arr[i][j][0] < 64:
      arr[i][j][0] = arr[i][j][1] = arr[i][j][2] = 32
    elif arr[i][j][0] >= 64 and arr[i][j][0] < 128:
      arr[i][j][0] = arr[i][j][1] = arr[i][j][2] = 96
    elif arr[i][j][0] >= 128 and arr[i][j][0] < 196:
      arr[i][j][0] = arr[i][j][1] = arr[i][j][2] = 162
    else:
      arr[i][j][0] = arr[i][j][1] = arr[i][j][2] = 255
img = Image.fromarray(arr)
img

#qn10

#10a
from PIL import ImageChops
im1 = Image.open("/content/drive/MyDrive/images/image9.webp") .convert("1")
im2 = Image.open("/content/drive/MyDrive/images/image1.png") .convert("1")
im3 = ImageChops.logical_and(im1, im2)
im3


#10b
im1 = Image.open("/content/drive/MyDrive/images/image9.webp") .convert("1")
im2 = Image.open("/content/drive/MyDrive/images/image1.png") .convert("1")
im3 = ImageChops.logical_or(im1, im2)
im3

#10c
im1 = Image.open("/content/drive/MyDrive/images/image9.webp") .convert("1")
im1

#ws connected component
import numpy as np
from matplotlib import pyplot as plt
imagea=np.random.randint(0,2,(8,8))

plt.gray()
plt.imshow(imagea)
plt.show()

def four_connectivity_labelling(image_array):
  labels = np.zeros([np.size(image_array, 0), np.size(image_array, 1)], dtype = int)
  label_count = 0
  object_count_deduct = 0
  equivalence_table= set()
  for i in range(len(image_array)):
    for j in range(len(image_array[i])):
     if image_array[i][j]==1:
       if i == 0 and j == 0:
        label_count+=1
        labels[i][j] = label_count
       elif i > 0 and j == 0:
        if image_array[i-1][j] == 1:
          labels[i][j] = labels[i-1][j]
        else:
          label_count+=1
          labels[i][j] = label_count
       elif i == 0 and j > 0:
        if image_array[i][j-1] == 1:
          labels[i][j] = labels[i][j-1]
        else:
          label_count+=1
          labels[i][j] = label_count
       else:
        if image_array[i-1][j] == 0 and image_array[i][j-1] == 1:
          labels[i][j] = labels[i][j-1]
        elif image_array[i-1][j] == 1 and image_array[i][j-1] == 0:
          labels[i][j] = labels[i-1][j]
        elif image_array[i][j-1] == 1 and image_array[i-1][j] == 1:
          labels[i][j] = min(labels[i-1][j], labels[i][j-1])
          if labels[i][j-1] != labels[i-1][j]:
            equivalence_table.add(max(labels[i-1][j], labels[i][j-1]))
        else:
          label_count+=1
          labels[i][j] = label_count

  print(labels)

four_connectivity_labelling(imagea)

#for real image 
import numpy as np
from matplotlib import pyplot as plt
from PIL import Image

image_m=Image.open("/content/figure.png")
image_m
image_m= image_m.convert('L')
image_m
BW= image_m.convert('1')
BW
img_arr = np.array(BW)
def four_connectivity_labelling(image_array):
  labels = np.zeros([np.size(image_array, 0), np.size(image_array, 1)], dtype = int)
  label_count = 0
  object_count_deduct = 0
  equivalence_table= set()
  for i in range(len(image_array)):
    for j in range(len(image_array[i])):
     if image_array[i][j]==1:
       if i == 0 and j == 0:
        label_count+=1
        labels[i][j] = label_count
       elif i > 0 and j == 0:
        if image_array[i-1][j] == 1:
          labels[i][j] = labels[i-1][j]
        else:
          label_count+=1
          labels[i][j] = label_count
       elif i == 0 and j > 0:
        if image_array[i][j-1] == 1:
          labels[i][j] = labels[i][j-1]
        else:
          label_count+=1
          labels[i][j] = label_count
       else:
        if image_array[i-1][j] == 0 and image_array[i][j-1] == 1:
          labels[i][j] = labels[i][j-1]
        elif image_array[i-1][j] == 1 and image_array[i][j-1] == 0:
          labels[i][j] = labels[i-1][j]
        elif image_array[i][j-1] == 1 and image_array[i-1][j] == 1:
          labels[i][j] = min(labels[i-1][j], labels[i][j-1])
          if labels[i][j-1] != labels[i-1][j]:
            equivalence_table.add(max(labels[i-1][j], labels[i][j-1]))
        else:
          label_count+=1
          labels[i][j] = label_count

  print(labels)
  # print(equivalence_table)
  # print("\nNumber of image objects are :- ", label_count - len(equivalence_table))

four_connectivity_labelling(img_arr)


#q2
import cv2
import numpy as np
import matplotlib.pyplot as plt

def connected_component_label(path):
    
    # Getting the input image
    img = cv2.imread(path, 0)
    # Converting those pixels with values 1-127 to 0 and others to 1
    img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)[1]
    # Applying cv2.connectedComponents() 
    num_labels, labels = cv2.connectedComponents(img)
    
    # Map component labels to hue val, 0-179 is the hue range in OpenCV
    label_hue = np.uint8(179*labels/np.max(labels))
    blank_ch = 255*np.ones_like(label_hue)
    labeled_img = cv2.merge([label_hue, blank_ch, blank_ch])

    # Converting cvt to BGR
    labeled_img = cv2.cvtColor(labeled_img, cv2.COLOR_HSV2BGR)

    # set bg label to black
    labeled_img[label_hue==0] = 0
    
    
    # Showing Original Image
    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.axis("off")
    plt.title("Orginal Image")
    plt.show()
    
    #Showing Image after Component Labeling
    plt.imshow(cv2.cvtColor(labeled_img, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.title("Image after Component Labeling")
    plt.show()
connected_component_label('/content/images.png')

#ws noise andannotation
import numpy as np
import cv2
from google.colab.patches import cv2_imshow

def add_salt_pepper_noise(image, prob):
    output = np.copy(image)
    thres = 1 - prob
    for i in range(image.shape[0]):
        for j in range(image.shape[1]):
            rdn = np.random.random()
            if rdn < prob:
                output[i][j] = 0
            elif rdn > thres:
                output[i][j] = 255
    return output

img = cv2.imread("bird.jpg")
print("\nOriginal Image\n")
cv2_imshow(img)
noisy_img = add_salt_pepper_noise(img, 0.05)
cv2.imwrite("noisy_salt_and_pepper_image.jpg", noisy_img)
print("\nSalt and pepper Noisy image\n")
cv2_imshow(noisy_img)

# Use Skimage library and generate Gaussian noise, Poisson noise, Salt and Pepper noise. Add these noise images to the original image and store them in different names.

import skimage
from skimage import io
import numpy as np
import os
from skimage import img_as_ubyte

img = io.imread("dog.jpg")
io.imshow(img)

def add_gaussian_noise(image, prob):
    gaussian_noise = skimage.util.random_noise(image, mode='gaussian', var=prob)
    io.imsave("gaussian_noise.jpg",img_as_ubyte(img))
    io.imshow(gaussian_noise)


print("\nGaussian Noise : \n")
add_gaussian_noise(img, 0.05)

def add_poisson_noise(image, prob):
    poisson_noise = skimage.util.random_noise(image, mode='poisson')
    io.imsave("poisson_noise.jpg", img_as_ubyte(poisson_noise))
    io.imshow(poisson_noise)

print("\nPoisson Noise : \n")
add_poisson_noise(img, 0.05)

def add_saltandpepper_noise(image, prob):
    salt_pepper_noise = skimage.util.random_noise(image, mode='s&p', amount=0.05)
    io.imsave("salt_pepper_noise.jpg", img_as_ubyte(salt_pepper_noise))
    io.imshow(salt_pepper_noise)

print("\nSalt and Pepper Noise : \n")
add_saltandpepper_noise(img, 0.05)


# For the newly generated images, apply mean filter, median filter and analyze the effectiveness.

def analyze_filters(img_paths):
    for img_path in img_paths:
        img = cv2.imread(img_path)
        
        # Mean filter
        mean_filter = cv2.blur(img,(3,3))
        mean_psnr = cv2.PSNR(img, mean_filter)
        print(f"PSNR for {img_path} with mean filter: {mean_psnr}")

        # Median filter
        median_filter = cv2.medianBlur(img,3)
        median_psnr = cv2.PSNR(img, median_filter)
        print(f"PSNR for {img_path} with median filter: {median_psnr}")

# Example usage
img_paths = ["gaussian_noise.jpg", "poisson_noise.jpg", "salt_pepper_noise.jpg"]
analyze_filters(img_paths)

#Text on Images

img = cv2.imread("flower.jpg")
imageText = img.copy()
text = 'This is a sample text'
org = (30,70)

# Text on the input image
cv2.putText(imageText, text, org, fontFace = cv2.FONT_HERSHEY_COMPLEX, fontScale = 0.8, color = (0,0,0))

cv2_imshow(imageText)

#RECTANGLE

imageRectangle = img.copy()

# starting and end points of the rectangle
start_point =(200,75)
end_point =(475,225)

cv2.rectangle(imageRectangle, start_point, end_point, (0, 0, 0), thickness= 3, lineType=cv2.LINE_8) 

cv2_imshow(imageRectangle)


#circle
imageCircle = img.copy()
imageFilledCircle = img.copy()
circle_center = (175,90)
radius =80
print("\nCircle\n")
cv2.circle(imageCircle, circle_center, radius, (0, 0, 0), thickness=3, lineType=cv2.LINE_AA) 
cv2_imshow(imageCircle)
print("\nFilled circle\n")
cv2.circle(imageFilledCircle, circle_center, radius, (0, 0, 0), thickness=-1, lineType=cv2.LINE_AA)
cv2_imshow(imageFilledCircle)


#line
imageLine = img.copy()
pointA = (5,220)
pointB = (250,220)
cv2.line(imageLine, pointA, pointB, (0, 0, 0), thickness=3)
cv2_imshow(imageLine)


#ellipse
imageEllipse = img.copy()
ellipse_center = (415,190)
axis1 = (75,40)
axis2 = (100,40)
cv2.ellipse(imageEllipse, ellipse_center, axis1, 0, 0, 360, (170, 51, 106), thickness=3)
cv2.ellipse(imageEllipse, ellipse_center, axis2, 90, 0, 360, (144, 238, 144), thickness=3)
cv2_imshow(imageEllipse)


#halfellipse
halfEllipse = img.copy()
ellipse_center = (315,190)

axis1 = (100,50)

cv2.ellipse(halfEllipse, ellipse_center, axis1, 0, 180, 360, (170, 51, 106), thickness=3)
cv2.ellipse(halfEllipse, ellipse_center, axis1, 0, 0, 180, (144, 238, 144), thickness=-2)
cv2_imshow(halfEllipse)

#ws color spacetransform and convolution

from PIL import Image
from google.colab import drive
drive.mount('/content/drive')

import cv2
from google.colab.patches import cv2_imshow
bright = cv2.imread('/content/drive/MyDrive/cube.png')
dark = cv2.imread('/content/drive/MyDrive/dark_cube.png')
cv2_imshow(bright)
print("\n")
cv2_imshow(dark)
#rgb to lab
brightLAB = cv2.cvtColor(bright, cv2.COLOR_BGR2LAB)
darkLAB = cv2.cvtColor(dark, cv2.COLOR_BGR2LAB)
cv2_imshow(brightLAB)
print("\n")
cv2_imshow(darkLAB)

#RGB To YCrCb
brightYCB = cv2.cvtColor(bright, cv2.COLOR_BGR2YCrCb)
darkYCB = cv2.cvtColor(dark, cv2.COLOR_BGR2YCrCb)
cv2_imshow(brightLAB)
print("\n")
cv2_imshow(darkLAB)


#RGB To HSV

brightHSV = cv2.cvtColor(bright, cv2.COLOR_BGR2HSV)
darkHSV = cv2.cvtColor(dark, cv2.COLOR_BGR2HSV)
cv2_imshow(brightLAB)
print("\n")
cv2_imshow(darkLAB)


#Create an array of size 15 x 15 containing gray intensity values. Consider another array of size 3 x 3 consisting of all 1's. Write a program to apply convolution.
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

image_array = np.random.randint(0, 255, (15,15))
filter_array = np.ones([3, 3], dtype = int)

plt.imshow(image_array)
plt.show()

convoluted_array = np.zeros([13, 13], dtype = int)

#mean filter
def convolute_image(image_array, i,j):
  sum=0
  for x in range(i- int(filter_array.shape[0]/2),i+ int(filter_array.shape[0]/2)):
    for y in range(j- int(filter_array.shape[1]/2),j+ int(filter_array.shape[1]/2)):
      sum+=image_array[x][y]
  return sum

for i in range(1, image_array.shape[0]-filter_array.shape[0]+2):
  for j in range(1, image_array.shape[1]-filter_array.shape[1]+2):
    convoluted_array[i-1][j-1] = convolute_image(image_array, i ,j)
plt.imshow(convoluted_array)
plt.show()

#edge detection


from google.colab.patches import cv2_imshow
import cv2

img = cv2.imread('/content/drive/MyDrive/dog.jpg') 
cv2_imshow(img)
cv2.waitKey(0)
img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

img_blur = cv2.GaussianBlur(img_gray, (3,3), 0) 
sobelx = cv2.Sobel(src=img_blur, ddepth=cv2.CV_64F, dx=1, dy=0, ksize=5) 
sobely = cv2.Sobel(src=img_blur, ddepth=cv2.CV_64F, dx=0, dy=1, ksize=5)
sobelxy = cv2.Sobel(src=img_blur, ddepth=cv2.CV_64F, dx=1, dy=1, ksize=5) 

cv2_imshow(sobelx)
cv2.waitKey(0)
cv2_imshow(sobely)
cv2.waitKey(0)
cv2_imshow(sobelxy)
cv2.waitKey(0)
edges = cv2.Canny(image=img_blur, threshold1=100, threshold2=200) 

cv2_imshow(edges)
cv2.waitKey(0)
cv2.destroyAllWindows()

#Canny edge detection

import numpy as np
import cv2
from google.colab.patches import cv2_imshow

image = cv2.imread("/content/drive/MyDrive/data/dog.jpg", cv2.IMREAD_GRAYSCALE)

"""# Gaussian Blurring"""

def gaussian(x, y, sigma):
  return (1 / (2 * np.pi * sigma ** 2)) * np.exp(-((x**2 + y**2) / (2 * sigma ** 2)))

def gaussian_kernel(size, sigma):
  k = size // 2
  kernel = np.zeros((size, size))
  for i in range(2 * k + 1):
    for j in range(2 * k + 1):
      kernel[i][j] = gaussian(i - k, j - k, sigma)
  
  return kernel

kernel = gaussian_kernel(5, 1)
image_gaussian = cv2.filter2D(src=image, ddepth=-1, kernel=kernel)

"""# Applying filter"""

image_sobel_x = cv2.filter2D(image_gaussian, ddepth=-1, kernel=np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]))
image_sobel_y = cv2.filter2D(image_gaussian, ddepth=-1, kernel=np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]))

"""# Non max suppression"""

magnitude = np.hypot(image_sobel_x, image_sobel_y)
magnitude = (magnitude / magnitude.max()) * 255
direction = np.arctan2(image_sobel_y, image_sobel_x)
direction = direction * 180 / np.pi
direction[direction < 0] += 180

cv2_imshow(magnitude)

image_suppressed = np.zeros((image.shape[0], image.shape[1]))
angle_direction_mapping = {
    (0, 22.5): (0, 1),
    (22.5, 67.5): (-1, -1),
    (67.5, 112.5): (1, 0),
    (112.5, 157.5): (1, -1),
    (157.5, 180): (0, 1)
}

for i in range(1, image.shape[0] - 1):
  for j in range(1, image.shape[1] - 1):
    current_pixel_value = magnitude[i][j]
    for angle, neighbour_direction in angle_direction_mapping.items():
      if angle[0] <= direction[i][j] < angle[1]:
        break
    neighbour1 = magnitude[i + neighbour_direction[0]][j + neighbour_direction[1]]
    neighbour2 = magnitude[i - neighbour_direction[0]][j - neighbour_direction[1]]
    if current_pixel_value >= neighbour1 and current_pixel_value >= neighbour2:
      image_suppressed[i][j] = current_pixel_value
    else:
      image_suppressed[i][j] = 0

# cv2_imshow((image_suppressed / image_suppressed.max()) * 255)
cv2_imshow(image_suppressed)

"""# Double Thresolding"""

low_thresh_ratio = 0.05
high_thresh_ratio = 0.09

STRONG = 255
WEAK = 25

high_thresh = image_suppressed.max() * high_thresh_ratio
low_thresh = high_thresh * low_thresh_ratio

thresholded_result = np.zeros(image.shape)
thresholded_result[image_suppressed > high_thresh] = STRONG
thresholded_result[(image_suppressed <= high_thresh) & (image_suppressed >= low_thresh)] = WEAK

cv2_imshow(thresholded_result)

"""# Edge tracking by Hysterisis"""

for i in range(1, image.shape[0]):
  for j in range(1, image.shape[1]):
    if thresholded_result[i][j] == WEAK:
      if (thresholded_result[i - 1:i + 2, j - 1 : j + 2] == STRONG).any():
        thresholded_result[i][j] = STRONG

cv2_imshow(thresholded_result)

#PCA

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import cv2
from scipy.stats import stats
import matplotlib.image as mpimg


img = cv2.cvtColor(cv2.imread('rose.jpg'), cv2.COLOR_BGR2RGB)
plt.imshow(img)
plt.show()

#Splitting into channels
blue,green,red = cv2.split(img)
# Plotting the images
fig = plt.figure(figsize = (15, 7.2)) 
fig.add_subplot(131)
plt.title("Blue Channel")
plt.imshow(blue)
fig.add_subplot(132)
plt.title("Green Channel")
plt.imshow(green)
fig.add_subplot(133)
plt.title("Red Channel")
plt.imshow(red)
plt.show()



blue_temp_df = pd.DataFrame(data = blue)
blue_temp_df

df_blue = blue/255
df_green = green/255
df_red = red/255


pca_b = PCA(n_components=50)
pca_b.fit(df_blue)
trans_pca_b = pca_b.transform(df_blue)
pca_g = PCA(n_components=50)
pca_g.fit(df_green)
trans_pca_g = pca_g.transform(df_green)
pca_r = PCA(n_components=50)
pca_r.fit(df_red)
trans_pca_r = pca_r.transform(df_red)


print(trans_pca_b.shape)
print(trans_pca_r.shape)
print(trans_pca_g.shape)


print(f"Blue Channel : {sum(pca_b.explained_variance_ratio_)}")
print(f"Green Channel: {sum(pca_g.explained_variance_ratio_)}")
print(f"Red Channel  : {sum(pca_r.explained_variance_ratio_)}")


fig = plt.figure(figsize = (15, 7.2)) 
fig.add_subplot(131)
plt.title("Blue Channel")
plt.ylabel('Variation explained')
plt.xlabel('Eigen Value')
plt.bar(list(range(1,51)),pca_b.explained_variance_ratio_)
fig.add_subplot(132)
plt.title("Green Channel")
plt.ylabel('Variation explained')
plt.xlabel('Eigen Value')
plt.bar(list(range(1,51)),pca_g.explained_variance_ratio_)
fig.add_subplot(133)
plt.title("Red Channel")
plt.ylabel('Variation explained')
plt.xlabel('Eigen Value')
plt.bar(list(range(1,51)),pca_r.explained_variance_ratio_)
plt.show()


#reconstruct
b_arr = pca_b.inverse_transform(trans_pca_b)
g_arr = pca_g.inverse_transform(trans_pca_g)
r_arr = pca_r.inverse_transform(trans_pca_r)
print(b_arr.shape, g_arr.shape, r_arr.shape)


img_reduced= (cv2.merge((b_arr, g_arr, r_arr)))
print(img_reduced.shape)


fig = plt.figure(figsize = (10, 7.2)) 
fig.add_subplot(121)
plt.title("Original Image")
plt.imshow(img)
fig.add_subplot(122)
plt.title("Reduced Image")
plt.imshow(img_reduced)
plt.show()